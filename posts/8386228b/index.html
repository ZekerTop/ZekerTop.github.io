<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Scrapy爬虫框架 | Ztop の 空间站</title><meta name="keywords" content="python爬虫之scrapy框架"><meta name="author" content="Ztop"><meta name="copyright" content="Ztop"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="python爬虫之scrapy框架">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy爬虫框架">
<meta property="og:url" content="https://www.zeker.top/posts/8386228b/index.html">
<meta property="og:site_name" content="Ztop の 空间站">
<meta property="og:description" content="python爬虫之scrapy框架">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/scrapy-1.52lo4766xv40.jpg">
<meta property="article:published_time" content="2021-09-04T12:58:53.000Z">
<meta property="article:modified_time" content="2022-01-10T04:18:17.781Z">
<meta property="article:author" content="Ztop">
<meta property="article:tag" content="python">
<meta property="article:tag" content="爬虫">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/scrapy-1.52lo4766xv40.jpg"><link rel="shortcut icon" href="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/touxiang3.4yrpzmedtq40.jpg"><link rel="canonical" href="https://www.zeker.top/posts/8386228b/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: {"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Scrapy爬虫框架',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2022-01-10 12:18:17'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    document.addEventListener('pjax:complete', detectApple)})(window)</script><link rel="stylesheet" href="css/background.css"><link rel="stylesheet" href="//at.alicdn.com/t/font_3130971_tze760ab8r9.css"><link rel="stylesheet" href="/css/custom.css"  media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper/swiper/swiper.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper/swiper/swiperstyle.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/css/main.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/l-lin/font-awesome-animation/dist/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://unpkg.zhimg.com/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script async src="https://cdn.jsdelivr.net/npm/hexo-butterfly-tag-plugins-plus@latest/lib/carousel-touch.min.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">加载中...</div></div></div><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/touxiang3.4yrpzmedtq40.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-fw fas fa-lightbulb"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-user-friends"></i><span> 朋友圈</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw iconfont icon-room-o"></i><span> 回声间</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/artitalk/"><i class="fa-fw iconfont icon-computerRoom"></i><span> 小黑屋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw iconfont icon-board"></i><span> 小黑板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 其他</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/scrapy-1.52lo4766xv40.jpg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">Ztop の 空间站</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 主页</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa-fw fas fa-lightbulb"></i><span> 关于</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-user-friends"></i><span> 朋友圈</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-clock"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw iconfont icon-room-o"></i><span> 回声间</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/artitalk/"><i class="fa-fw iconfont icon-computerRoom"></i><span> 小黑屋</span></a></li><li><a class="site-page child" href="/comments/"><i class="fa-fw iconfont icon-board"></i><span> 小黑板</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> 其他</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music/"><i class="fa-fw fas fa-music"></i><span> 音乐</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Scrapy爬虫框架</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2021-09-04T12:58:53.000Z" title="发表于 2021-09-04 20:58:53">2021-09-04</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2022-01-10T04:18:17.781Z" title="更新于 2022-01-10 12:18:17">2022-01-10</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/python/">python</a><i class="fas fa-angle-right post-meta-separator"></i><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/python/%E7%88%AC%E8%99%AB/">爬虫</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">3.6k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>13分钟</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Scrapy爬虫框架"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="Scrapy框架概念">Scrapy框架概念</h2>
<p><code>Scrapy</code>是一个Python编写的开源网络爬虫框架。它是一个被设计用于爬取网络数据、提取结构性数据的框架。</p>
<p>Scrapy文档地址：<a target="_blank" rel="noopener" href="http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html">http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html</a></p>
<h2 id="Scrapy框架作用">Scrapy框架作用</h2>
<p>少量的代码，就能够快速的抓取。一般用于爬取大量数据。</p>
<h2 id="Scrapy框架工作流程">Scrapy框架工作流程</h2>
<ol>
<li>
<p>回顾request的爬虫流程</p>
<p><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/%E5%8E%9F%E5%A7%8B%E7%9A%84%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B.1a7d9jv58bmo.jpg" alt="原始的爬虫流程"></p>
<p>我们可以在此基础上改写流程：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/%E6%94%B9%E5%86%99%E7%9A%84%E7%88%AC%E8%99%AB%E6%B5%81%E7%A8%8B.ef49avaji3k.png" alt="改写的爬虫流程"></p>
<p>而上面改写的流程图也更加便于大家去理解scrapy的流程</p>
</li>
<li>
<p>Scapy的流程</p>
<p><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/scapy%E7%9A%84%E6%B5%81%E7%A8%8B.2rcpyfhjjoo0.png" alt="scapy的流程"></p>
<h3 id="其流程详细如下：">其流程详细如下：</h3>
<ol>
<li>爬虫中起始的url构造成request对象——&gt;爬虫中间件——&gt;引擎——&gt;调度器</li>
<li>调度器把request——&gt;引擎——&gt;下载中间件——&gt;下载器</li>
<li>下载器发送请求，获取response响应——&gt;下载中间件——&gt;引擎——&gt;爬虫中间件——&gt;爬虫</li>
<li>爬虫提取url地址，组装成request对象——&gt;爬虫中间件——&gt;引擎——&gt;调度器，重复步骤2</li>
<li>爬虫提取数据——&gt;引擎——&gt;管道处理和保存数据</li>
</ol>
</li>
</ol>
<div class="note warning no-icon flat"><p>​	注意：</p>
<ul>
<li>图中绿色线条的表示数据的传递</li>
<li>注意图中中间件的位置，决定了其作用</li>
<li>注意其中引擎的位置，所有的模块之前相互独立，只和引擎进行交互</li>
</ul>
</div>
<h2 id="各模块的具体作用">各模块的具体作用</h2>
<p><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/%E5%90%84%E6%A8%A1%E5%9D%97%E4%BD%9C%E7%94%A8.5z8n2kdpqtw0.jpg" alt="各模块作用"></p>
<p>​	各模块功能：</p>
<ul>
<li>引擎 —— 数据和信号的传递</li>
<li>调度器 —— 任务url队列</li>
<li>下载器 —— 发送请求、获取响应</li>
<li>爬虫 —— 起始的url、解析数据</li>
<li>管道 —— 保存数据</li>
<li>中间件 —— 定制化操作</li>
</ul>
<h2 id="三个内置对象">三个内置对象</h2>
<ul>
<li>request请求对象：由url、method、post_data、headers等构成</li>
<li>response响应对象：由url、body、status、headers等构成</li>
<li>item数据对象：本质是个字典</li>
</ul>
<h2 id="安装">安装</h2>
<p>有时pip版本过于老旧不能使用，需要升级pip版本，输入<code>pip install --upgrade pip</code>回车，升级成功</p>
<p>安装scrapy命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip/pip3 install Scrapy</span><br></pre></td></tr></table></figure>
<h2 id="scrapy项目开发流程">scrapy项目开发流程</h2>
<ol>
<li>创建项目</li>
<li>生成一个爬虫</li>
<li>提取数据</li>
<li>保存数据</li>
</ol>
<h2 id="创建项目">创建项目</h2>
<p>创建scrpy项目的命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject &lt;项目名字&gt;</span><br></pre></td></tr></table></figure>
<p>示例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myspider</span><br></pre></td></tr></table></figure>
<h2 id="创建爬虫">创建爬虫</h2>
<div class="note info flat"><p>通过命令创建出爬虫文件，爬虫文件为主要的代码作业文件，通常一个网站的爬取动作都会在爬虫文件中进行编写。</p>
</div>
<p>命令：在<strong>项目路径下</strong>执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider &lt;爬虫名字&gt; &lt;允许爬取的域名&gt;</span><br></pre></td></tr></table></figure>
<ul>
<li>爬虫名字：作为爬虫运行时的参数</li>
<li>允许爬取的域名：为对于爬虫设置的爬取范围，设置之后用于过滤要爬取的url，如果爬取的url与允许的域不通则被过滤掉。如不确定时，<a target="_blank" rel="noopener" href="http://xn--xx-hf3c34q0p7dlpp.com">可以设置xx.com</a>，后期再进行修改。</li>
</ul>
<p>这里我们以<code>豆瓣电影Top250</code>作为示例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd myspider</span><br><span class="line">scrapy genspider douban movie.douban.com</span><br></pre></td></tr></table></figure>
<p>生成的目录和文件结果如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/%E7%9B%AE%E5%BD%95%E6%96%87%E4%BB%B6%E6%80%BB%E8%A7%88.5z4b3r667x00.jpg" alt="目录文件总览"></p>
<h2 id="完善爬虫">完善爬虫</h2>
<p>在上一步生成出来的爬虫文件中编写指定网站的数据采集操作，实现数据提取</p>
<p><strong>一、在item.py中定义要提取的字段</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyspiderItem</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    num = scrapy.Field()	<span class="comment"># 电影序号   </span></span><br><span class="line">    name = scrapy.Field()	<span class="comment"># 电影名字</span></span><br><span class="line">    score = scrapy.Field()	<span class="comment"># 电影评分   </span></span><br><span class="line">    con = scrapy.Field()	<span class="comment"># 电影简介</span></span><br></pre></td></tr></table></figure>
<p><strong>二、在/myspider/myspider/spiders/douban.py 中修改内容如下</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> MyspiderItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubanSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;douban&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;movie.douban.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://movie.douban.com/top250&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="comment"># 电影名字</span></span><br><span class="line">        name = response.xpath(<span class="string">&#x27;.//div[@id=&quot;content&quot;]/div/div/ol/li[*]/div/div/div/a/span[1]/text()&#x27;</span>).getall()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 电影评分</span></span><br><span class="line">        score = response.xpath(<span class="string">&#x27;.//*[@id=&quot;content&quot;]/div/div[1]s/ol/li[*]/div/div/div/div/span[3]/@content&#x27;</span>).getall()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 美化格式</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">25</span>):</span><br><span class="line">            <span class="comment">#在爬虫中导入并且实例化对象, 使用方法跟使用字典类似</span></span><br><span class="line">            item = MyspiderItem()</span><br><span class="line">            item[<span class="string">&#x27;name&#x27;</span>] = name[i]	<span class="comment"># 这里的键名要跟item.py中字段名一致</span></span><br><span class="line">            item[<span class="string">&#x27;score&#x27;</span>] = score[i]</span><br><span class="line">            <span class="built_in">print</span>(item)</span><br><span class="line">            <span class="keyword">yield</span> item      <span class="comment"># yield 会把数据传给管道</span></span><br></pre></td></tr></table></figure>
<div class="note warning no-icon flat"><p>注意：</p>
<ul>
<li>scrapy.Spider爬虫类中必须有名为parse的解析</li>
<li>如果网站结构层次比较复杂，也可以自定义其他解析函数</li>
<li>在解析函数中提取的url地址如果要发送请求，则必须属于allowed_domains范围内，但是start_urls中的url地 址不受这个限制，我们会在后续的课程中学习如何在解析函数中构造发送请求</li>
<li>启动爬虫的时候注意启动的位置，是在项目路径下启动</li>
<li>parse()函数中使用yield返回数据，注意：解析函数中的yield能够传递的对象只能是：BaseItem, Request, dict, None</li>
</ul>
</div>
<p><strong>三、定位元素以及提取数据、属性值的方法</strong></p>
<div class="note info flat"><p>解析并获取scrapy爬虫中的数据: 利用xpath规则字符串进行定位和提取</p>
</div>
<ol>
<li>
<p>response.xpath方法的返回结果是一个类似list的类型，其中包含的是selector对象，操作和列表一样，但是有 一些额外的方法</p>
</li>
<li>
<p>额外方法extract()：返回一个包含有字符串的列表（相当于getall() ）</p>
</li>
<li>
<p>额外方法extract_first()：返回列表中的第一个字符串，列表为空没有返回None（相当于get() ）</p>
</li>
</ol>
<p><strong>四、response响应对象的常用属性</strong></p>
<ul>
<li>response.url：当前响应的url地址</li>
<li>response.request.url：当前响应对应的请求的url地址</li>
<li>response.headers：响应头</li>
<li>response.requests.headers：当前响应的请求头</li>
<li>response.body：响应体，也就是html代码，byte类型</li>
<li>response.status：响应状态码</li>
</ul>
<p><strong>五、（改进版）可构造Request对象，并发送请求</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> ..items <span class="keyword">import</span> MyspiderItem</span><br><span class="line"></span><br><span class="line">num = <span class="number">0</span>     <span class="comment"># 全局变量</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubanSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;douban&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;movie.douban.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://movie.douban.com/top250&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        <span class="comment"># 电影名字</span></span><br><span class="line">        name = response.xpath(<span class="string">&#x27;.//div[@id=&quot;content&quot;]/div/div/ol/li[*]/div/div/div/a/span[1]/text()&#x27;</span>).getall()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 电影评分</span></span><br><span class="line">        score = response.xpath(<span class="string">&#x27;.//*[@id=&quot;content&quot;]/div/div[1]/ol/li[*]/div/div/div/div/span[2]/text()&#x27;</span>).getall()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 电影链接</span></span><br><span class="line">        link = response.xpath(<span class="string">&#x27;.//*[@id=&quot;content&quot;]/div/div[1]/ol/li[*]/div/div[2]/div[1]/a/@href&#x27;</span>).getall()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 美化格式</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">25</span>):</span><br><span class="line">            item = MyspiderItem()</span><br><span class="line">            <span class="keyword">global</span> num</span><br><span class="line">            <span class="keyword">if</span> num == i:        <span class="comment"># 防止乱序</span></span><br><span class="line">                item[<span class="string">&#x27;num&#x27;</span>] = num+<span class="number">1</span></span><br><span class="line">                item[<span class="string">&#x27;name&#x27;</span>] = name[i]</span><br><span class="line">                item[<span class="string">&#x27;score&#x27;</span>] = score[i]</span><br><span class="line">                <span class="comment"># print(item)</span></span><br><span class="line">                <span class="comment"># yield item      # yield 会把数据传给管道</span></span><br><span class="line"></span><br><span class="line">                <span class="comment"># print(link[i])</span></span><br><span class="line">                <span class="comment"># 对获取的电影链接去发送请求</span></span><br><span class="line">                <span class="keyword">yield</span> scrapy.Request(link[i], callback=self.parse_data, meta=&#123;<span class="string">&#x27;item2&#x27;</span>: item&#125;)</span><br><span class="line">                num += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 翻页操作一</span></span><br><span class="line">        next_url = response.xpath(<span class="string">&#x27;.//*[@id=&quot;content&quot;]/div/div[1]/div[2]/span[3]/a/@href&#x27;</span>).get()</span><br><span class="line">        <span class="comment"># 拼接</span></span><br><span class="line">        next_url = <span class="string">&#x27;https://movie.douban.com/top250&#x27;</span> + next_url</span><br><span class="line"></span><br><span class="line">        <span class="comment"># # 翻页操作二</span></span><br><span class="line">        <span class="comment"># # n_url = response.xpath(&quot;//a[text()=&#x27;后页&gt;&#x27;]/@href&quot;)</span></span><br><span class="line">        <span class="comment"># # 直接使用response携带残缺的url</span></span><br><span class="line">        <span class="comment"># # yield response.follow(n_url, callback=self.parse)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 手动构造请求对象，指定解析起始url的parse方法</span></span><br><span class="line">        <span class="comment"># yield scrapy.Request(next_url, callback=self.parse)   # 数据过多，防止被反爬，先注释掉  </span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解析详情的函数(自定义)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_data</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        item = response.meta.get(<span class="string">&#x27;item2&#x27;</span>)   <span class="comment"># 或 item = response.meta[&#x27;item2&#x27;]</span></span><br><span class="line">        <span class="comment"># print(item)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 提取详情简介</span></span><br><span class="line">        <span class="comment"># 有的节点不一样，有些是div/span   有些div/span/span</span></span><br><span class="line">        <span class="comment"># // *[ @ id = &quot;link-report&quot;] / span[1] / text()</span></span><br><span class="line">        <span class="comment"># // *[ @ id = &quot;link-report&quot;] / span[1] / span / text()</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># content = response.xpath(&#x27;//*[@id=&quot;link-report&quot;]/span[1]/span/text()&#x27;).get()</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># string(path) 方法会提取父标签下的文本内容</span></span><br><span class="line">        <span class="comment"># path  就是父标签的路径</span></span><br><span class="line">        content = response.xpath(<span class="string">&#x27;string(.//*[@id=&quot;link-report&quot;]/span)&#x27;</span>).get()</span><br><span class="line">        <span class="comment"># print(&#x27;详情:&#x27;, content)</span></span><br><span class="line">        item[<span class="string">&#x27;con&#x27;</span>] = content.strip()  <span class="comment"># strip()去除左右两边的空格</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">yield</span> item</span><br></pre></td></tr></table></figure>
<p>scrapy.Request()中的常见参数解释</p>
<table>
<thead>
<tr>
<th>参数</th>
<th>解释</th>
<th>是否必填</th>
</tr>
</thead>
<tbody>
<tr>
<td>url</td>
<td>请求的url</td>
<td>是</td>
</tr>
<tr>
<td>callback</td>
<td>回调函数，用于接收请求后的返回信息，若没指定，则默认为parse()函数</td>
<td>是</td>
</tr>
<tr>
<td>meta</td>
<td>方法之间以字典形式传递参数，这个参数一般也可在<code>middlewares</code>中处理</td>
<td>否</td>
</tr>
<tr>
<td>method</td>
<td>http请求的方式，默认为GET请求，一般不需要指定。若需要POST请求，建议使用用<code>scrapy.FormRequest()</code></td>
<td>否</td>
</tr>
<tr>
<td>headers</td>
<td>dict类型，请求头信息，一般在settings中设置即可，也可在middlewares中设置</td>
<td>否</td>
</tr>
<tr>
<td>cookies</td>
<td>dict或list类型，请求的cookie</td>
<td>否</td>
</tr>
<tr>
<td>dont_filter</td>
<td>是否开启过滤，默认关闭，开启之后爬取过的url,下一次不会再爬取</td>
<td>否</td>
</tr>
<tr>
<td>errback</td>
<td>抛出错误的回调函数并打印出来，错误包括404，超时，DNS错误等</td>
<td>否</td>
</tr>
</tbody>
</table>
<div class="note success no-icon flat"><p>发送post请求</p>
<p><code>scrapy.FormRequest(url，callback, formdata)</code></p>
<p>FormRequest 类为Request的子类，用于POST请求，其他参数与Request一样，其中新增的<code>formdata</code>是dict类型，相当于meta。</p>
</div>
<h2 id="保存数据">保存数据</h2>
<div class="note info flat"><p>利用管道pipeline来处理（保存）数据</p>
</div>
<p><strong>一、在pipelines.py文件中定义对数据的操作</strong></p>
<ol>
<li>定义一个管道类</li>
<li>重写管道类的process_item方法</li>
<li>process_item方法处理完item之后必须返回给引擎</li>
<li>定义数据的保存逻辑</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyspiderPipeline</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.file = <span class="built_in">open</span>(<span class="string">&#x27;douban.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">        <span class="comment"># with open(&#x27;douban250.txt&#x27;, &#x27;a&#x27;, encoding=&#x27;utf-8&#x27;) as f:</span></span><br><span class="line">        <span class="comment">#     f.write(str(item) + &#x27;\n&#x27;)</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 把传递数据的载体item对象转为一个字典</span></span><br><span class="line">        dic = <span class="built_in">dict</span>(item)</span><br><span class="line">        js_data = json.dumps(dic, ensure_ascii=<span class="literal">False</span>) + <span class="string">&#x27;\n&#x27;</span></span><br><span class="line">        self.file.write(js_data)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">        self.file.close()</span><br></pre></td></tr></table></figure>
<ul>
<li>def open_spider(self, spider) —— 爬虫<code>开启</code>时执行一次，可用来<code>打开</code>文件</li>
<li>def process_item(self, item, spider) ——实现数据的写入操作</li>
<li>def close_spider(self, spider) —— 爬虫<code>关闭</code>时执行一次，可用来<code>关闭</code>文件</li>
</ul>
<p><strong>二、<a target="_blank" rel="noopener" href="http://xn--settings-q86n.py">在settings.py</a> 配置启用管道</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启用管道配置</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">&#x27;myspider.pipelines.MyspiderPipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绕过robots规则，直接爬取页面</span></span><br><span class="line"><span class="comment"># Obey robots.txt rules</span></span><br><span class="line">ROBOTSTXT_OBEY = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加User-agent</span></span><br><span class="line"><span class="comment">#USER_AGENT = &#x27;myspider (+http://www.yourdomain.com)&#x27;</span></span><br><span class="line">USER_AGENT = <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加cookie</span></span><br><span class="line">COOKIES_ENABLED = <span class="literal">False</span></span><br><span class="line">DEFAULT_REQUEST_HEADERS = &#123;</span><br><span class="line">  <span class="string">&#x27;Accept&#x27;</span>: <span class="string">&#x27;text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;Accept-Language&#x27;</span>: <span class="string">&#x27;en&#x27;</span>,</span><br><span class="line">  <span class="string">&#x27;Cookie&#x27;</span>: <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>setting.py一般都会将<code>管道配置</code>注释掉，取消注释即可。</p>
<p>配置项中键为使用的管道类，管道类使用<code>.</code>进行分割，第一个为<code>项目目录</code>，第二个为<code>文件</code>，第三个为<code>定义的管道类</code>。 配置项中值为管道的使用顺序，设置的数值<code>越小越优先执行</code>，该值一般设置为1000以内。</p>
<h2 id="运行爬虫">运行爬虫</h2>
<p>命令：在<strong>项目目录下</strong>执行</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl &lt;爬虫名字&gt;	(--nolog)</span><br></pre></td></tr></table></figure>
<div class="note orange icon flat"><i class="note-icon fas fa-battery-half"></i><p>—nolog：不显示调试信息，不加即默认显示</p>
</div>
<p>示例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl douban --nolog</span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/运行结果.5s2ykwcn5bk0.jpg" alt="运行结果" style="zoom:67%;" />
<h2 id="crawlspider爬虫">crawlspider爬虫</h2>
<p>回顾之前的代码，有很多一部分时间都寻找下一页的url地址或者内容的url地址上面，而这个过程能更简单吗？</p>
<p>需求思路：</p>
<ol>
<li>从response中提取所有的满足规则的url地址</li>
<li>自动的构造自己requests请求，发送给引擎</li>
</ol>
<p>而<code>crawlspider</code>就可以满足上述需求，能够匹配满足条件的url地址，组装成Reuqest对象后自动发送给引擎， 同时能够指定callback函数，</p>
<p>即：<u>crawlspider爬虫可以按照规则自动获取连接</u></p>
<div class="note info no-icon flat"><p>Scrapy框架中分两类爬虫，Spider类和CrawlSpider类。CrawlSpider继承自spider，只不过是在之前的基础上增加了新的功能。可以定义爬取的url的规则，以后scrapy碰到满足条件的url都进行爬取，而不用手动yield Request。</p>
</div>
<h2 id="创建crawlspider爬虫并观察爬虫内的默认内容">创建crawlspider爬虫并观察爬虫内的默认内容</h2>
<p><strong>一、创建crawlspider爬虫：</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy genspider -t crawl douban movie.douban.com</span><br></pre></td></tr></table></figure>
<p><strong>二、spider中默认生成的内容如下：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DoubnSpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">    name = <span class="string">&#x27;douban&#x27;</span></span><br><span class="line">    allowed_domains = [<span class="string">&#x27;movie.douban.com&#x27;</span>]</span><br><span class="line">    start_urls = [<span class="string">&#x27;http://movie.douban.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">    rules = (</span><br><span class="line">        Rule(LinkExtractor(allow=<span class="string">r&#x27;Items/&#x27;</span>), callback=<span class="string">&#x27;parse_item&#x27;</span>, follow=<span class="literal">True</span>),</span><br><span class="line">    )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">        item = &#123;&#125;</span><br><span class="line">        <span class="comment">#item[&#x27;domain_id&#x27;] = response.xpath(&#x27;//input[@id=&quot;sid&quot;]/@value&#x27;).get()</span></span><br><span class="line">        <span class="comment">#item[&#x27;name&#x27;] = response.xpath(&#x27;//div[@id=&quot;name&quot;]&#x27;).get()</span></span><br><span class="line">        <span class="comment">#item[&#x27;description&#x27;] = response.xpath(&#x27;//div[@id=&quot;description&quot;]&#x27;).get()</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>
<p><strong>三、观察其与跟普通的scrapy.spider的区别</strong></p>
<div class="note danger flat"><p>在crawlspider爬虫中，没有parse函数</p>
</div>
<p>重点在<code>rules</code>中：</p>
<ol>
<li>rules是一个元组或者是列表，包含的是Rule对象</li>
<li>Rule表示规则，其中包含LinkExtractor，callback和follow等参数</li>
<li>LinkExtractor:连接提取器，可以通过<code>正则</code>或者是<code>xpath</code>来进行url地址的匹配</li>
<li>callback :表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调函数 的处理</li>
<li>follow：连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，True表示会，Flase表示不会</li>
</ol>
<p><strong>四、crawlspider使用的注意点</strong></p>
<ol>
<li>除了用命令 <code>scrapy genspider -t crawl &lt;爬虫名&gt;  &lt;allowed_domail&gt;</code>创建一个crawlspider的模板，页可以手动创建</li>
<li>crawlspider中<code>不能</code>再有以<code>parse</code>为名的数据提取方法，该方法被crawlspider用来实现基础url提取等功能</li>
<li>Rule对象中LinkExtractor为固定参数，其他callback、follow为可选参数</li>
<li>不指定callback且follow为True的情况下，满足rules中规则的url还会被继续提取和请求</li>
<li>如果一个被提取的url满足多个Rule，那么会从rules中选择一个满足匹配条件的Rule执行</li>
</ol>
<p><strong>五、crawlspider其他知识点的了解</strong></p>
<ol>
<li>链接提取器<code>LinkExtractor</code>的更多常见参数
<ul>
<li>allow：满足括号中的’re’表达式的url会被提取，如果为空，则全部匹配</li>
<li>deny：满足括号中的’re’表达式的url不会被提取，优先级高于allow</li>
<li>allow_domains：会被提取的链接的domains(url范围)，如： [‘<a target="_blank" rel="noopener" href="http://hr.tencent.com">hr.tencent.com</a>’, ‘<a target="_blank" rel="noopener" href="http://baidu.com">baidu.com</a>’]</li>
<li>deny_domains：不会被提取的链接的domains(url范围)</li>
<li>restrict_xpaths：使用xpath规则进行匹配，和allow共同过滤url，即xpath满足的范围内的url地址会被 提取，如： restrict_xpaths=‘//div[@class=“pagenav”]’</li>
</ul>
</li>
<li><code>Rule</code>常见参数
<ul>
<li>LinkExtractor：链接提取器，可以通过正则或者是xpath来进行url地址的匹配</li>
<li>callback：表示经过连接提取器提取出来的url地址响应的回调函数，可以没有，没有表示响应不会进行回调 函数的处理</li>
<li>follow：连接提取器提取的url地址对应的响应是否还会继续被rules中的规则进行提取，默认True表示会， Flase表示不会</li>
<li>process_links：当链接提取器LinkExtractor获取到链接列表的时候调用该参数指定的方法，这个自定义方 法可以用来过滤url，且这个方法执行后才会执行callback指定的方法</li>
</ul>
</li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Ztop</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://www.zeker.top/posts/8386228b/">https://www.zeker.top/posts/8386228b/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://www.zeker.top" target="_blank">Ztop の 空间站</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a></div><div class="post_share"><div class="social-share" data-image="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/scrapy-1.52lo4766xv40.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><link rel="stylesheet" href="/css/coin.css" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">打赏</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="/img/wechat.jpg" target="_blank"><img class="post-qr-code-img" src="/img/wechat.jpg" alt="微信"/></a><div class="post-qr-code-desc">微信</div></li><li class="reward-item"><a href="/img/alipay.jpg" target="_blank"><img class="post-qr-code-img" src="/img/alipay.jpg" alt="支付宝"/></a><div class="post-qr-code-desc">支付宝</div></li></ul></div></button></div><audio id="coinAudio" src="https://unpkg.zhimg.com/akilar-candyassets@1.0.16/audio/coin.mp3"></audio><script defer="defer" src="/js/coin.js"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/posts/6cb57fa2/"><img class="prev-cover" src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210809/redi3-3.6h3pj74p2b80.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Redis学习笔记（3）</div></div></a></div><div class="next-post pull-right"><a href="/posts/510672c5/"><img class="next-cover" src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell1.110bz2j3f3xc.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">WebShell（1）| 基础详解</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts-list"><div><a href="/posts/7ee00516/" title="针对eduSrc平台制作搜索脚本"><img class="cover" src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220208/wallhaven-83153y.1411bt8maduo.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-06</div><div class="title">针对eduSrc平台制作搜索脚本</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/touxiang3.4yrpzmedtq40.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">Ztop</div><div class="author-info__description">去更远的地方，见更亮的光</div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">12</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">标签</div><div class="length-num">10</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">分类</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/ZekerTop"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/ZekerTop" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:top.zeker@gmail.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/ONE_ZJC?spm=1000.2115.3001.5343" target="_blank" title="csdn"><i class="iconfont icon-csdn1"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>公告</span></div><div class="announcement_content">学习路上永不止境！！！<img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/my_bg.74vya3aoiik0.gif"></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E6%A1%86%E6%9E%B6%E6%A6%82%E5%BF%B5"><span class="toc-number">1.</span> <span class="toc-text">Scrapy框架概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E6%A1%86%E6%9E%B6%E4%BD%9C%E7%94%A8"><span class="toc-number">2.</span> <span class="toc-text">Scrapy框架作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E6%A1%86%E6%9E%B6%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-number">3.</span> <span class="toc-text">Scrapy框架工作流程</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E6%B5%81%E7%A8%8B%E8%AF%A6%E7%BB%86%E5%A6%82%E4%B8%8B%EF%BC%9A"><span class="toc-number">3.1.</span> <span class="toc-text">其流程详细如下：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%84%E6%A8%A1%E5%9D%97%E7%9A%84%E5%85%B7%E4%BD%93%E4%BD%9C%E7%94%A8"><span class="toc-number">4.</span> <span class="toc-text">各模块的具体作用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E4%B8%AA%E5%86%85%E7%BD%AE%E5%AF%B9%E8%B1%A1"><span class="toc-number">5.</span> <span class="toc-text">三个内置对象</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85"><span class="toc-number">6.</span> <span class="toc-text">安装</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#scrapy%E9%A1%B9%E7%9B%AE%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B"><span class="toc-number">7.</span> <span class="toc-text">scrapy项目开发流程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="toc-number">8.</span> <span class="toc-text">创建项目</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E7%88%AC%E8%99%AB"><span class="toc-number">9.</span> <span class="toc-text">创建爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%8C%E5%96%84%E7%88%AC%E8%99%AB"><span class="toc-number">10.</span> <span class="toc-text">完善爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BF%9D%E5%AD%98%E6%95%B0%E6%8D%AE"><span class="toc-number">11.</span> <span class="toc-text">保存数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%BF%90%E8%A1%8C%E7%88%AC%E8%99%AB"><span class="toc-number">12.</span> <span class="toc-text">运行爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#crawlspider%E7%88%AC%E8%99%AB"><span class="toc-number">13.</span> <span class="toc-text">crawlspider爬虫</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAcrawlspider%E7%88%AC%E8%99%AB%E5%B9%B6%E8%A7%82%E5%AF%9F%E7%88%AC%E8%99%AB%E5%86%85%E7%9A%84%E9%BB%98%E8%AE%A4%E5%86%85%E5%AE%B9"><span class="toc-number">14.</span> <span class="toc-text">创建crawlspider爬虫并观察爬虫内的默认内容</span></a></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/posts/7ee00516/" title="针对eduSrc平台制作搜索脚本"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220208/wallhaven-83153y.1411bt8maduo.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="针对eduSrc平台制作搜索脚本"/></a><div class="content"><a class="title" href="/posts/7ee00516/" title="针对eduSrc平台制作搜索脚本">针对eduSrc平台制作搜索脚本</a><time datetime="2022-02-06T03:41:26.000Z" title="发表于 2022-02-06 11:41:26">2022-02-06</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/77d4ef29/" title="WebShell（3）| 木马上传方式及查杀防范"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell4.4f7zzmqep940.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WebShell（3）| 木马上传方式及查杀防范"/></a><div class="content"><a class="title" href="/posts/77d4ef29/" title="WebShell（3）| 木马上传方式及查杀防范">WebShell（3）| 木马上传方式及查杀防范</a><time datetime="2022-01-20T12:29:15.000Z" title="发表于 2022-01-20 20:29:15">2022-01-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/8a6b0e5b/" title="WebShell（2）| 内存马（不死马）"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell3.3k6c1wm41va0.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WebShell（2）| 内存马（不死马）"/></a><div class="content"><a class="title" href="/posts/8a6b0e5b/" title="WebShell（2）| 内存马（不死马）">WebShell（2）| 内存马（不死马）</a><time datetime="2022-01-16T14:46:09.000Z" title="发表于 2022-01-16 22:46:09">2022-01-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/510672c5/" title="WebShell（1）| 基础详解"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell1.110bz2j3f3xc.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="WebShell（1）| 基础详解"/></a><div class="content"><a class="title" href="/posts/510672c5/" title="WebShell（1）| 基础详解">WebShell（1）| 基础详解</a><time datetime="2022-01-16T03:54:40.000Z" title="发表于 2022-01-16 11:54:40">2022-01-16</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/posts/8386228b/" title="Scrapy爬虫框架"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/scrapy-1.52lo4766xv40.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Scrapy爬虫框架"/></a><div class="content"><a class="title" href="/posts/8386228b/" title="Scrapy爬虫框架">Scrapy爬虫框架</a><time datetime="2021-09-04T12:58:53.000Z" title="发表于 2021-09-04 20:58:53">2021-09-04</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/scrapy-1.52lo4766xv40.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2022 <i style="color:#FF6A6A;animation: announ_animation 0.8s linear infinite;" class="fa fa-heartbeat"></i> Ztop</div><div id="workboard"></div><script async="async" src="/js/runtime.js"></script><div class="footer_custom_text"><p> <a style="margin-inline:5px"target="_blank" href="https://hexo.io/"> <img src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为 Hexo" alt="HEXO"></a> <a style="margin-inline:5px"target="_blank" href="https://butterfly.js.org/"> <img src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用 Butterfly" alt="Butterfly"></a> <a style="margin-inline:5px"target="_blank" href="https://www.jsdelivr.com/"> <img src="https://img.shields.io/badge/CDN-jsDelivr-green?style=flat&logo=jsDelivr" title="本站使用 JsDelivr 为静态资源提供CDN加速" alt="JsDelivr"></a> <a style="margin-inline:5px"target="_blank" href="https://twikoo.js.org/"> <img src="https://img.shields.io/badge/Comment-Twikoo-orange?style=flat&logo=Vercel" title="Twikoo 提供评论支持" alt="Twikoo"></a> <a style="margin-inline:5px"target="_blank" href="https://github.com/"> <img src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由 GitHub 托管" alt="GitHub"></a> <a style="margin-inline:5px"target="_blank" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"> <img src="https://img.shields.io/badge/Copyright-BY--NC--SA%204.0-d42328?style=flat&logo=Claris" title="本站采用知识共享署名-非商业性使用-相同方式共享4.0国际许可协议进行许可" alt="BY--NC--SA4.0"></a> <br> <img src="https://s1.ax1x.com/2018/09/29/ilmwIH.png"> <a href="https://beian.miit.gov.cn"  style="color:#f0d784" target="_blank">粤ICP备2021107291号-1</a> </p></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><script src="/js/search/local-search.js"></script><script>var preloader = {
  endLoading: () => {
    document.body.style.overflow = 'auto';
    document.getElementById('loading-box').classList.add("loaded")
  },
  initLoading: () => {
    document.body.style.overflow = '';
    document.getElementById('loading-box').classList.remove("loaded")

  }
}
window.addEventListener('load',preloader.endLoading())</script><div class="js-pjax"><script>(()=>{
  const $countDom = document.getElementById('twikoo-count')
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://twikoo-ten-red.vercel.app/',
      region: ''
    }, null))
  }

  const getCount = () => {
    twikoo.getCommentsCount({
      envId: 'https://twikoo-ten-red.vercel.app/',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      $countDom.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const loadTwikoo = (bool = false) => {
    if (typeof twikoo === 'object') {
      init()
      bool && $countDom && setTimeout(getCount,0)
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(()=> {
        init()
        bool && $countDom && setTimeout(getCount,0)
      })
    }
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo(true)
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://twikoo-ten-red.vercel.app/',
        region: '',
        pageSize: 6,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><div class="aplayer no-destroy" data-id="6751453557" data-server="tencent" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="false" muted></div><script async src="//at.alicdn.com/t/font_2264842_3izu8i5eoc2.js"></script><script defer="defer" id="fluttering_ribbon" mobile="true" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-show-text" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-show-text.min.js" data-mobile="false" data-text="富强,民主,文明,和谐,自由,平等,公正,法治,爱国,敬业,诚信,友善" data-fontsize="15px" data-random="false" async="async"></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = [
  'title',
  '#config-diff',
  '#body-wrap',
  '#rightside-config-hide',
  '#rightside-config-show',
  '.js-pjax'
]

if (false) {
  pjaxSelectors.unshift('meta[property="og:image"]', 'meta[property="og:title"]', 'meta[property="og:url"]')
}

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.removeEventListener('scroll', window.tocScrollFn)
  window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // Analytics
  if (false) {
    MtaH5.pgv()
  }

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && location.pathname =='/'){
    
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220208/wallhaven-83153y.1411bt8maduo.jpg" alt="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220208/wallhaven-83153y.1411bt8maduo.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2022-02-06</span><a class="blog-slider__title" href="posts/7ee00516/">针对eduSrc平台制作搜索脚本</a><div class="blog-slider__text">eduSrc平台搜索脚本</div><a class="blog-slider__button" href="posts/7ee00516/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell4.4f7zzmqep940.jpg" alt="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell4.4f7zzmqep940.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2022-01-20</span><a class="blog-slider__title" href="posts/77d4ef29/">WebShell（3）| 木马上传方式及查杀防范</a><div class="blog-slider__text">木马有多少种上传方式，又怎样去查杀预防呢？</div><a class="blog-slider__button" href="posts/77d4ef29/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell1.110bz2j3f3xc.jpg" alt="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell1.110bz2j3f3xc.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2022-01-16</span><a class="blog-slider__title" href="posts/510672c5/">WebShell（1）| 基础详解</a><div class="blog-slider__text">对webShell一个基本详解以及木马分类</div><a class="blog-slider__button" href="posts/510672c5/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell3.3k6c1wm41va0.jpg" alt="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20220110/webshell3.3k6c1wm41va0.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2022-01-16</span><a class="blog-slider__title" href="posts/8a6b0e5b/">WebShell（2）| 内存马（不死马）</a><div class="blog-slider__text">内存马的学习以及改进思路</div><a class="blog-slider__button" href="posts/8a6b0e5b/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/scrapy-1.52lo4766xv40.jpg" alt="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210906/scrapy-1.52lo4766xv40.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2021-09-04</span><a class="blog-slider__title" href="posts/8386228b/">Scrapy爬虫框架</a><div class="blog-slider__text">python爬虫之scrapy框架</div><a class="blog-slider__button" href="posts/8386228b/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210809/redis3-2.3d402rvhn140.jpg" alt="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210809/redis3-2.3d402rvhn140.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2021-08-07</span><a class="blog-slider__title" href="posts/1bb24f34/">Redis学习笔记（2）</a><div class="blog-slider__text">Redis的主从复制与集群详解</div><a class="blog-slider__button" href="posts/1bb24f34/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210806/7db5b53fd5e2724a961934f9bc2430f.6qn1wzw6j4g0.jpg" alt="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210806/7db5b53fd5e2724a961934f9bc2430f.6qn1wzw6j4g0.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2021-08-06</span><a class="blog-slider__title" href="posts/b921ad40/">Spring boot整合Redis</a><div class="blog-slider__text">Jedis、Spring-data-redis的配置以及区别</div><a class="blog-slider__button" href="posts/b921ad40/">详情</a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms"><div class="blog-slider__img"><img src="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210809/redi3-3.6h3pj74p2b80.jpg" alt="https://cdn.jsdelivr.net/gh/ZekerTop/images@main/20210809/redi3-3.6h3pj74p2b80.jpg"/></div><div class="blog-slider__content"><span class="blog-slider__code">2021-08-10</span><a class="blog-slider__title" href="posts/6cb57fa2/">Redis学习笔记（3）</a><div class="blog-slider__text">Redis缓存异常问题</div><a class="blog-slider__button" href="posts/6cb57fa2/">详情</a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载swiper')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script>
<script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper/swiper/swiper.min.js"></script>
<script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-swiper@0.18/swiper/swiperindex.js"></script>
<style></style><script data-pjax>function history_calendar_injector_config(){
                var parent_div_git = document.getElementsByClassName('sticky_layout')[0];
                var item_html = '<div class="card-widget card-history"><div class="card-content"><div class="item-headline"><i class="fas fa-clock fa-spin"></i><span>那年今日</span></div><div id="history-baidu" style="height: 100px;overflow: hidden"><div class="history_swiper-container" id="history-container" style="width: 100%;height: 100%"><div class="swiper-wrapper" id="history_container_wrapper" style="height:20px"></div></div></div></div>';
                console.log('已挂载history_calendar')
                // parent_div_git.innerHTML=item_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",item_html) // 有报错，但不影响使用(支持pjax跳转)
            }if( document.getElementsByClassName('sticky_layout')[0] && (location.pathname ==='all'|| 'all' ==='all')){

            history_calendar_injector_config()
        } </script><script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/Butterfly-card-history/baiduhistory/js/main.js"></script>
  <script data-pjax src="https://cdn.jsdelivr.net/gh/Zfour/hexo-github-calendar@1.21/hexo_githubcalendar.js"></script>
  <script data-pjax>
        function GithubCalendarConfig(){
            var git_githubapiurl ="https://python-github-calendar-api.vercel.app/api?ZekerTop";
            var git_color =['#f2f8fc', '#e2f0fa', '#d6ebfa', '#c8e1ff', '#add9f7', '#97cefa', '#53b1f0', '#3dabf5', '#1E90FF', '#4169E1', '#0000CD'];
            var git_user ="ZekerTop";
            var parent_div_git = document.getElementById('recent-posts');
            var git_div_html = '<div class="recent-post-item" style="width:100%;height:auto;padding:10px;"><div id="github_loading" style="width:10%;height:100%;margin:0 auto;display: block"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"  viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animateTransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animateTransform></path></svg></div><div id="github_container"></div></div>';
            if(parent_div_git && location.pathname =='/'){
                console.log('已挂载github calendar')
                // parent_div_git.innerHTML=git_div_html+parent_div_git.innerHTML // 无报错，但不影响使用(支持pjax跳转)
                parent_div_git.insertAdjacentHTML("afterbegin",git_div_html) // 有报错，但不影响使用(支持pjax跳转)
            };
            GithubCalendar(git_githubapiurl,git_color,git_user)
        }
        if(document.getElementById('recent-posts')){
            GithubCalendarConfig()
        }
    </script>
    <style>#github_container{min-height:280px}@media screen and (max-width:650px) {#github_container{background-image:;min-height:0px}}</style>
    <style></style><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>